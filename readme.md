# Topics

- Intro to transformer arch
  - Core component : Attention mechanism
  - self attention (scaled dot product attention)
  - cross attention mechanism
  - multi head attention
  - Local attention
  - Linform attention
  - Longform attention
  - performer attention
  - Performer attention
  - Flash attention
  - Reformer
  - Mega (Mixture of Experts with Gated Attention)
  - Grouped-query attention (GQA)
  - Multi-query attention (MQA)
  - Sliding window attention
  - Dynamic Convolution
  - Hyena Hierarchies 
