# Topics

- Intro to transformer arch
  - Core component : Attention mechanism
  - self attention (scaled dot product attention)
  - cross attention mechanism
  - multi head attention
  - Local attention
  - Linform attention
  - Longform attention
  - performer attention
